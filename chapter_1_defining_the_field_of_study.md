# Chapter 1: Defining the Field of Study

## What’s in a Name

In our view, data journalism as a field encompasses a suite of practices for collecting, analyzing, visualizing, and publishing data for journalistic purposes. This definition may well be debated. The history of data journalism is full of arguments about what it should be called and what it includes.

In fact, data journalism has been evolving ever since CBS used a computer to successfully predict the outcome of the presidential election in 1952\. As technology has advanced, so has the ability of journalists to tap that technology and use it for important storytelling.

One key definition of data journalism can be found in a 2014 report by Alexander Howard for the Tow Center for Digital Journalism and Knight Foundation. Data journalism is “gathering, cleaning, organizing, analyzing, visualizing, and publishing data to support the creation of acts of journalism,” Howard wrote. “A more succinct definition might be simply the application of data science to journalism, where data science is defined as the study of the extraction of knowledge from data.”2

But news games, drone journalism, and virtual reality—approaches that some may not consider mainstream data journalism today—may represent a much more dominant presence tomorrow. Or data journalism may evolve in yet another direction, perhaps into common applications for machine learning and algorithms. Data journalists are already working more with unstructured information (text, video, audio) as opposed to the historical elements of data journalism (spreadsheets and databases full of rows and columns of numbers).

“I think the one good thing about the name discussion is that people are realizing there are different kinds of approaches to data for journalism,” said Brant Houston, the Knight Chair in Investigative and Enterprise Reporting at the University of Illinois at Urbana-Champaign and a former executive director of Investigative Reporters and Editors (IRE).

The ever-evolving practice of data journalism has at heart represented what journalists do best—push against the boundaries of what is expected. Editors used to argue that readers wouldn’t understand a scatterplot published in the newspaper. Today, the New York Times’s Upshot, Fivethirtyeight.com, and others regularly provide informative data graphics and visualizations.

At the same time, each generation of data journalists has informed the next and balanced the desire to try new methods with foundational ethics and transparency.

Our study aims to provide a broad evaluation of many areas of journalistic practice involving data and to identify best practices for teaching these skills and the “data frame of mind” that goes with them. In doing so we looked at multiple forms of data journalism and defined them as best we could to ensure clear communication.

## Four Key Areas of Data journalism

For this report, we will divide data journalism into four categories, acknowledging that overlap is inevitable in practice. Examples of journalism that fall under each of these headings can be found in the appendix.

### Data Reporting

definition: Obtaining, cleaning, and analyzing data for use in telling journalistic stories.

#### Includes:

*   Deploying computer-assisted reporting or analysis for writing journalistic stories
*   Practicing precision journalism, as introduced by Philip Meyer, including the use of social science research methods in the interest of journalism
*   Visualizing data—mapping and charting—for use in exploration and analysis
*   Programming to obtain and analyze data for writing journalistic stories

#### Techniques and Technologies:

*   Invoking public records law to negotiate for data
*   Using web scraping tools and techniques (ranges from tools to knowledge of Python programming language)
*   Using relational database software (can range from Microsoft Access to MySQL)
*   Understanding statistical concepts and software or programming languages with statistical packages (SPSS or R among others)
*   Using mapping and visualization tools and software (Tableau, Esri mapping software, QGIS, Google Fusion)

### Data Visualization and Interactives

definition: Using code for digital publishing (HTML/CSS/JavaScript/jQuery) as well as programming and database management to build interactive journalistic work. This overlaps with design work, which falls outside of traditional definitions of data journalism. But visualizations and apps also can be integral to the storytelling process.

#### Includes:

*   Visualizations developed and designed as interactive charts and graphics for presentation, including the use of code
*   Interactive applications, including searchable databases and games that help readers explore and understand a news story; these applications can be a key part of the utility of a data journalism project

#### Techniques and Technologies:

*   The use of code, which is defined as HTML and CSS and also could include JavaScript
*   The use of visualization software or programs, ranging from Tableau visualizations to the D3 JavaScript Library
*   Database management and programming, including Python, web frameworks such Django, Flask and Ruby on Rails, and more
*   Mapping applications, including QGIS, CartoDB, Esri, TileMill, GeoDjango, and more
*   Server knowledge and the use of GitHub, versioning, and Agile software development techniques

### Emerging Journalistic Technologies

#### Definition: New developments using data and technology.

*   Drone Journalism
*   Sensor Journalism
*   Virtual and Augmented Reality Journalism

#### Drone Technologies:

“Drone journalism is generally defined as the use of unmanned aerial systems to gather photos, video and data for news. What separates Drone Journalism from drone photography is the application of journalistic ethics and consideration of the public interest when using [drones].” — Matt Waite, a professor of practice at the University of Nebraska and founder of the Drone Journalism Lab

Drone technologies can include an airframe, defined by configuration (such as fixed wing or multirotor); an autopilot of varying capabilities (full automation, minor stability assistance, return-to-home fail-safe functionality); a control system (manual control through radio signals, automated flight through software and Bluetooth wireless connection); and a sensor (camera, video camera, multispectral camera, other physical sensor).

#### Sensor Technologies:

Sensor technologies include a wide range of software and hardware to measure physical conditions like air quality, motion, or noise levels. These can be used to gather data with a small, portable computer or microcontroller. The Raspberry Pi is a low-cost, credit card–sized computer that has a variety of input/output pins for mounting devices like sensors. Similarly, Arduino is an open-source microcontroller platform that is widely used for prototyping with electronic components like sensors. Some universities have already begun teaching sensor journalism with specific project-based classes, such as to test environmental conditions like air and water quality.

#### Virtual and Augmented Reality Technologies:

Virtual reality (VR), long heralded as an emerging digital technology, finally appears poised to enter the broad consumer market. Samsung, Oculus, and Google have developed consumer VR headsets along with controllers to facilitate interactivity using your hands and feet. From a production standpoint, panoramic images and videos may be stitched together from an array of cameras, while the company Jaunt is developing a standalone camera to capture 3D video in 360-degree, immersive format. Yet questions of narrative, audience interaction, and journalistic values have yet to be settled with these technologies, even as the New York Times, Los Angeles Times, and PBS “Frontline” have launched exploratory ventures to use VR. Journalism schools need to not only provide exposure and instruction in this emerging technology, but also to inquire into values and best practices.

### Computational Journalism

definition: The use of algorithms, machine learning, and other new methods to accomplish journalistic goals. This area overlaps with data reporting and emerging technologies.

#### Includes:

*   Algorithms that help journalists mine unstructured data in new ways
*   New digital platforms to better manage documents and data

#### Technologies:

*   Programming languages like Python, Ruby, and R
*   Frameworks and applications like Jupyter that enable journalists to mix code and prose as they perform analysis and show the steps in their work
*   Platforms like Overview that facilitate the use of complicated computational processes like natural language processing and topic modeling

## A Brief History of Computers and Journalists

In 1967, Philip Meyer had just returned to Knight Ridder’s Washington Bureau from a Nieman Fellowship at Harvard University, where he had delved into a different area of computational methods: social science. Social science methodologies, including statistical tests and surveys, had recently been used by academics to detail the reasons behind the 1965 Watts riots in Los Angeles. Meyer believed similar methodologies could have great impact in journalism. He wasn’t back at work for long when he was able to put that belief into practice.

In July 1967, an early morning raid of an unlicensed bar in Detroit resulted in rioting. Crowds of people ran through the streets, burning, looting, and shooting. Theories abounded as to why the rioting had occurred. Some experts thought it was done by those “on the bottom rung of society” with no money or education. A second theory was that it was caused by transplanted and unassimilated Southerners.

Meyer, on loan to Knight Ridder’s Detroit Free Press, reached out to friends who were social scientists to devise a survey, cobble together funding, and train interviewers. In the survey, respondents, who were guaranteed anonymity, were asked to assess their own level of participation in the riots. They were also asked to indicate whether they considered rioting a crime, whether they supported fines or jail for the looters, and whether they considered African Americans in Detroit to be better off than those elsewhere.

The survey results contradicted the earlier theories and pointed to a different explanation—that the relative good fortune of many African Americans highlighted more deeply the gap felt by those who were left behind.

The Free Press’s coverage of the rioting, including Meyer’s “swift and accurate investigation into the underlying causes,” won the Pulitzer Prize for Local General Reporting in 1968 and launched a new era in the use of computational methods in the service of journalism. Meyer’s seminal book, Precision Journalism: A Reporter’s Introduction to Social Science Methods was published in 1973 and argued that journalists trained in social science methods would be better equipped for journalistic work and provided guidelines for journalists to understand those methods.3 “The tools of sampling, computer analysis, and statistical inference increased the traditional power of the reporter without changing the nature of his or her mission,” Meyer wrote, “to find the facts, to understand them, and to explain them without wasting time.”4

That pioneering work by Meyer is commonly thought to be the beginning of what has been termed either precision journalism or computer-assisted reporting. His approach inspired other journalists. Their work in turn inspired a movement and the creation of a training ground. Two academic institutions in particular, Indiana University and the University of Missouri, supported the development of that training ground.

But in the wider academic world, computational methods applied to reporting largely did not have an impact on other university programs or how journalism was taught. Instead, professional journalists taught other professional journalists the new techniques, and only as those data journalists began to enter academia did data journalism education begin to take a wider hold in that setting.

By the 1980s, as desktop personal computers took the place of typewriters, and editing terminals were used with digital publishing systems, reporters began to use software on PCs to great effect. In 1986, Elliot Jaspin, a reporter at the Providence Journal-Bulletin, used databases to match felons and bad driving records to school bus drivers.

In 1988, Bill Dedman, a reporter for the Atlanta Journal Constitution, using data from a 9-track tape and with analysis by Dwight Morris and input from the Hubert H. Humphrey School of Public Affairs at the University of Minnesota, showed that banks were redlining African Americans on loans throughout Atlanta, and eventually the country, while providing services in even the poorest white neighborhoods. That series, “The Color of Money,” won a Pulitzer Prize in Investigative Reporting.

By 1989, Jaspin launched the Missouri Institute for Computer-Assisted Reporting (MICAR) at the University of Missouri. Soon, he was teaching computer-assisted reporting to students at the university and holding boot camps for professional journalists. Four years later, in 1994, a Freedom Forum grant would help the institute boost its presence and become a part of IRE as NICAR—the National Institute for Computer-Assisted Reporting.

In 1990, at Indiana University, former journalist turned professor James Brown worked with IRE to organize the first computer-assisted reporting conference, sponsored by IRE. He created a fledgling group called the National Institute for Advanced Reporting (NIAR).

“Andy Schneider, a two-time Pulitzer winner, had just joined our faculty as the first Riley Chair professor. One day we were talking about how so few journalists used computers in their reporting,” Brown recalled in an email. “In 1990, I don’t know of any schools that had such skills integrated into the curriculum. At that time, any undergraduate in even the smallest school of business knew how to use a spreadsheet. We decided to do something about it and that was how NIAR started.”

NIAR would host six conferences before deciding to fold to avoid duplicating efforts by IRE and MICAR, Brown said. Still, the Indiana conferences trained more than 1,000 journalists and were a precursor to a new era. In 1993, IRE and MICAR (which later would be renamed to NICAR), held a computer-assisted reporting conference in Raleigh, North Carolina, that drew several hundred attendees. That marked the beginning of an annual event that continues today, where new generations of reporters and editors learn to use spreadsheets or query data and to use maps and statistics to arrive at newsworthy findings.

In 1993, the same year as the Raleigh computer-assisted reporting conference, the Miami Herald received the Pulitzer Prize for Public Service after reporter Steve Doig used data analysis and mapping to show that weakened building requirements were the reason Hurricane Andrew had so devastated certain parts of Miami.

Much of this new computer-assisted reporting came about because as the Internet emerged and became more accessible, so too did the concept of using a computer in reporting. But NICAR and the University of Missouri in particular had a broad and deep impact. A good number of the most prominent practitioners of data journalism learned their skills from NICAR and from other journalists trying to solve similar data challenges.

This pattern is perhaps most visible through tracking the careers of the NICAR trainers themselves. Sarah Cohen was part of a Washington Post team that received the 2002 Pulitzer Prize in investigative reporting for detailing the District of Columbia’s role in the neglect and death of 229 children in protective care, and Jennifer LaFleur has won multiple national awards for the coverage of disability, legal, and open government issues. Both were NICAR trainers.

Another NICAR trainer was Tom McGinty, now a reporter at the Wall Street Journal and the data journalist for “Medicare Unmasked,” which received the 2015 Pulitzer Prize in Investigative Reporting. Jo Craven McGinty was also a NICAR trainer and later worked as a database specialist at the Washington Post and at the New York Times; she now writes a data-centric column for the Wall Street Journal. Her analysis about the use of lethal force by Washington police was part of a Post series that received the Pulitzer Prize for Public Service and the Selden Ring Award for Investigative Reporting in 1999.

Journalist David Donald moved on from his NICAR training role to head data efforts at the Center for Public Integrity and is now data editor at American University’s Investigative Reporting Workshop.

Aron Pilhofer was an IRE/NICAR trainer and led IRE’s campaign finance information center. He went on to work at the Center for Public Integrity and the New York Times, where he founded the paper’s first interactives team. Today, Pilhofer is digital executive editor at the Guardian.

Justin Mayo, a data journalist at the Seattle Times, graduated from the University of Missouri and worked in the NICAR database library and as a NICAR trainer. He has paired with reporters on work that has opened sealed court cases and changed state laws governing logging permits. Mayo was involved in data analysis and reporting on an investigative project on problems with prescription methadone policies in the state of Washington, which received a Pulitzer Prize for Investigative Reporting in 2012 and in covering a mudslide that received a Pulitzer Prize for Breaking News Reporting in 2015.

Clearly, working at NICAR has meant building powerful skills. So, too, has attending conferences and boot camps. The students who attended early NICAR boot camps were “missionaries” who returned to their newsrooms to teach computational journalism skills to their colleagues, Houston recalled. For years, the conferences and boot camps were “the only place where people have had an extensive amount of time to try out new techniques.”5

By the late 1990s, as the increasing prominence of the Internet led more news organizations to post stories online, journalism education offered even more digitally focused instruction: multimedia, online video skills, and HTML coding, among others.

Two strands, data and digital, represent distinct uses of computers within journalism. Early calls for journalism schools to adapt to changing technological conditions were answered mainly with the addition of digital classes—learning how to build a web page, create multimedia, and curate content.

Many of the early digitally focused journalism instructors faced a battle in trying to introduce new concepts into print journalism traditions. Data journalism instructors—focusing more on data analysis for use in stories—have faced similar challenges.

Meanwhile, by the 1990s, a few universities had begun teaching data analysis for storytelling. Meyer, who in 1981 became Knight Chair at the University of North Carolina, was teaching statistical analysis as a reporting method. Indiana University, with Brown, the professor who launched the first CAR conference, began incorporating the methods into classes. And Missouri offered computer-assisted reporting instruction, thanks to Jaspin; Brant Houston, an early NICAR director who later became IRE’s executive director; and others. Other universities began to introduce basic classes or incorporate spreadsheets into existing classes.

Houston’s Computer-Assisted Reporting: A Practical Guide became one of the few foundational texts available on the subject. His book, now in its fourth edition, lays out the basics of computer-assisted reporting: working with spreadsheets and database managers as well as finding data that can be used for journalism, such as local budgets and bridge inspection information. What Houston detailed in that first edition became essentially a core curriculum for data journalism from 1995 through the present day. Houston’s work codified the principles and practices of computer-assisted reporting from the perspective of its burgeoning community.

But throughout those two decades, journalists still learned these skills primarily through the NICAR conferences or from other journalists. For many years, for example, Meyer and Cohen taught a NICAR stats and maps boot camp at the University of North Carolina geared toward teaching professional journalists.

Since then, boot camps have become a popular model, used by universities and other journalism training organizations, often in coordination with IRE/NICAR. A key tenet of the boot camp is practical, hands-on training, using data sets that journalists routinely report on, such as school test scores. To sum up this model, Houston said it’s all about “learning by doing.”

Many boot camp graduates have gone on to robust data journalism careers and have also moved into teaching in journalism programs, both as adjuncts and full-time faculty, where they have integrated those teaching techniques into their classes. These journalists essentially took the curriculum from NICAR and introduced it into the wider academic world.

In 1996, Arizona State University lured Doig from the Miami Herald to the academic life where he has been teaching data journalism ever since, serving as the Knight Chair in Journalism and specializing in data journalism. The stats and maps boot camp eventually migrated to ASU as well.

As journalism programs began to offer these classes, they focused on the basics covered in Houston’s book: negotiating for data, cleaning it, and using spreadsheets and relational databases, mapping, and statistics to find stories.

In 2005, ASU benefited from a push by the Carnegie Corporation of New York and the John S. and James L. Knight Foundation to revamp journalism education. The school expanded its focus on all things data and multimedia with the founding of News21\. That program has focused heavily on using data to tell important and far-reaching stories while teaching hundreds of students journalism at the same time.

At Columbia, the first course on computer-assisted reporting was offered in 2003, when Tom Torok, then data editor at the New York Times, taught a one-credit elective. With the founding of the Stabile Center for Investigative Journalism in 2006, some data-driven reporting methods were integrated into the coursework for the small group of students selected for the program. The number of offerings in data and computation at Columbia has risen steadily since the founding of the Tow Center for Digital Journalism in 2010 and the Brown Institute for Media Innovation in 2012\. In addition to research and technology development projects, these centers brought full-time faculty and fellows to teach data and computation, as well as supplied grants to support the creation of new journalistic platforms and modes of storytelling.

Columbia has also launched several new programs in recent years that situate data and computational skills within journalistic practice. One is a dual-degree program in which students simultaneously pursue M.S. degrees in both Journalism and Computer Science — and those students must be admitted to both programs independently. In 2014, the Columbia Journalism School established a second data program, The Lede, in part to aid students in developing the broad skillset they would need to be a competitive applicant to both Journalism and CS. The Lede is a non-degree program that provides an intensive introduction to data and computation over the course of one or two semesters. Most students arrive with little or no experience with programming or data analysis, but after three to six months they emerge with a working knowledge of how databases, algorithms, and visualization can be put to narrative use. Post Lede, many students are competitive applicants for the dual degree, but others go directly into the field as reporters.

The emergence of these initiatives in journalism schools reflects the extent to which data-driven reporting practices have broadened in the last decade. In the 2000s, journalists began to move well beyond CAR, trying out advanced statistical analysis techniques, crowdsourcing in ways that ensured data accuracy and verification, web scraping, programming, and app development.

In 2009, IRE began working to attract programmers and journalists specializing in data visualization, said executive director Mark Horvit. It always offered hands-on sessions in analyzing data, mapping, and statistical methods. Added to that now are sessions on web scraping, multiple programming languages, web frameworks, and data visualization, among other topics. The sessions have even included drone demonstrations. The challenge has become balancing the panels so that there is enough of each type of data journalism. As a result, the annual conferences have grown tremendously, from around 400 at the CAR conference each year in the early 2000s to between 900 and 1,000 attendees today.

Other groups began addressing data journalism as well as pushing for new methods of digital journalism. The Society of Professional Journalists wanted to teach its members about data and joined with IRE to do so, sponsoring regional two- or three-day Better Watchdog Workshops. Minority journalism associations began to provide data journalism training, often in collaboration with IRE or its members or under the Better Watchdog theme.

The Online News Association’s annual conference focuses on the larger world of digital journalism. Many of its panels feature coding for presentation, cutting-edge developments in digital web-based products, audience development, and mobile. It also offers panels on data journalism and programming.

Still, a gap has persisted. At times, new organizations formed to fill some of the needs. In 2009, Pilhofer, then at the New York Times, Rich Gordon from Northwestern University, and Associated Press correspondent Burt Herman, who was just finishing a Knight Fellowship at Stanford, created a loosely knit organization that brings together journalists and technologists, hence the name Hacks/Hackers. Its mission is to create a network of people who “rethink the future of news and information.” Even as some groups have tried to fill gaps in data journalism instruction, what exactly counts as data journalism remains a rough boundary, with few distinctions between data journalism and digital/web skills. In this paper, we continue to sharpen the focus on what will improve the level of data journalism education, not overall digital instruction.

In 2013, a group of journalists used Kickstarter to raise $34,000 and create ForJournalism.com, a teaching platform to provide tutorials on spreadsheets, scraping, building apps, and visualizations. Founder Dave Stanton said the group wanted to focus on teaching programmatic journalism concepts and skills and offer subjects that weren’t being taught. “You didn’t really even have these online code school things,” he said. “There were a few. The problem was there was no context for journalism.”

## The Task at Hand: Causes for Concern and Reasons for Hope

With data coursework lacking in so many schools, the strongest presence of data journalism in most of academia has been the study of changing newsrooms by sociologists and communication scholars. Their work aims to document and explain data practices within ongoing scholarly conversations about media, technology, information, and society.

Elsewhere in academia, narrative uses of data and computation have emerged independently. Besides the work of quantitative social scientists, like those who inspired the work of Meyer, significant movements in the arts and humanities treat data either as a novel inroad to their traditional objectives or as a means to reinterpret those objectives. Probably the broadest of these movements falls under the heading of the “digital humanities.” One of its leading figures, Franco Moretti of Stanford University’s English department, has developed methods of “distant reading” by which one asks questions of a set of books larger than any one person could read in a lifetime. Dennis Tenen, a professor in Columbia University’s English and Comparative Literature department who has also taught at the Journalism School, identifies himself as a practitioner of computational cultural studies and argues that most disciplines have by now developed computational methods that have either complemented or supplanted their earlier practices.6

Several universities have founded centers and institutes devoted to work at the nexus of data, computation, and humanistic endeavors. The University of Illinois, Urbana-Champaign, for instance, hosts the Institute for Computing in Humanities, Arts, and Social Sciences, or I-CHASS, a partnership between the university and the National Center for Supercomputing Applications. The institute helps develop partnerships among social scientists and computing experts, engineers, data scientists, and computer scientists. Their collaborations have included work on large-scale video analysis, research into climate change, and even digitizing and analyzing the papers of Abraham Lincoln.

The uses of data and computation in architecture, geography, and economics also reflect the manner in which these disciplines adopted new tools and methods in recent decades. In journalism, our history is not so different. Like data journalism, computational work in the humanities and social sciences is growing, and this is reflected in the relatively healthy academic job market for digital humanists compared with the job market for traditional scholars.

Overall, we see data science and computational methods being introduced into disciplines across universities that, like journalism, have not been particularly quantitative in the past. Practices involving the use of data and computational methods may be bundled into entirely new departments, centers, research institutes, and degree programs (such as data science and computational media). It is not the purpose of a program in data journalism to compete with these other disciplines, but to develop a curriculum that is intrinsically journalistic—one that reflects a mission to find and tell stories in the public interest—as well as develop partnerships and collaborations with other disciplines.

One example of unexpected interdepartmental collaboration at Columbia has been with the Earth Institute, which has curated a massive database of climate data and offers courses in Python programming in which several Journalism students have enrolled. This course focuses on large time-series data sets, which enables data journalists to put the climate into context in their stories.

In 2013, Jean Folkerts, John Maxwell Hamilton, and Nicholas Lemann—all journalism school deans and two of the three of them longtime professional journalists—published “Educating Journalists: A New Plea for the University Tradition.” The paper focused on “universities’ role in journalism as a profession” but it also discussed how this transformation in journalism could be a boon for the schools that educate journalists. The authors wrote:

That journalism is going through profound changes does not vitiate—in fact, it enhances—the importance of journalism schools’ becoming more fully participant in the university project. Done properly, that will produce many benefits for the profession at a critical time. Journalism schools should be oriented toward the future of the profession as well as the present, and they should not be content merely to train their students in prevailing entry-level newsroom practices.7

Key among their recommendations was this: “We see all three of these early strains in journalism education—practice-oriented, subject matter-oriented, and research-oriented—as essential. And all of them can and should be applied, with potentially rich results, to the digital revolution. Journalism schools should embrace all three, not choose one and reject the others.”8

Journalism programs, with their ability to communicate to a general audience and their potential to analyze and visualize data for story, are a perfect partner for other departments. For example, at Stanford’s new Computational Journalism Lab (co-founded by one of this report’s authors), faculty are working on several projects with professors from other academic disciplines whose research mission touches on the same data. One goal is that data sets can be collected, analyzed, and used in academic research as well as for journalistic storytelling. In some instances, new methods of analysis can be developed in concert with important public accountability journalism projects.

Talk to deans of journalism schools today and you will hear the same refrain and the belief that data journalism, while not a savior, is an increasingly important component of how journalism education can evolve.

Steve Coll, the dean of the Columbia Graduate School of Journalism, describes the emergence of instruction in data-driven reporting practices as a recognition that data journalism is about more than just publishing stories through digital media, but about developing reporting methods appropriate to the complexity of the world today.

“Data journalism and tools like sensors look powerful because, in comparison to the way journalism schools have responded to previous iterations of technological change, this one runs deep, and to the heart of professional practice. It’s not about shifting distribution channels, or shifting structures of audience,” Coll said. “It was very tempting, in many ways necessary, for journalism schools to rush over to the teaching of tools, the teaching of platforms, the teaching of changing audience structure. But that transformation often had little to do with the core, enduring purpose of journalism, which is to discover, illuminate, hold power to account, explain, illustrate.”

Journalism schools, by necessity, adapted many new tools to respond to the massive and rapid shift to digital media. But delving into data journalism brings journalism back to its journalistic mission and moves it ahead in its research mission at the same time, Coll said.

“What we’re really seeing now is that this is a durable change in the structure of information, and therefore a need to durably change a journalist’s knowledge in order to carry out their core democratic function. Not to build a business model, not to reach more people, not to have more followers, but to actually discover the truth—you need to learn this.”

The rise of data analysis may also foster cross-campus collaboration. Journalism schools, as they embrace data analysis within their already powerful ability to tell stories, are uniquely suited to be robust participants and even leaders in developing means of storytelling with data.

Our research, which is focused on journalism schools, may not account for programs where data analysis is centered in another school or department that teaches this subject to students throughout the university. For undergraduates, in particular, there is little reason to offer in-house classes in subjects that students have free rein to study in another department. Yet it would require a great deal of latitude and initiative for students to construct hybrid degrees this way. Journalism students can sometimes be better served by cross-departmental initiatives that pair instructors for team teaching and connect journalism students with other disciplines that focus on data and computation. Northwestern, Stanford, Boston University, Columbia, Georgia Tech, Syracuse, and others have worked to build these interdisciplinary initiatives.

By establishing these interdepartmental bridges, schools can create pathways of collaboration between journalism, its partner disciplines of communication and media studies, and the other areas of research that share an interest in the future of technology and society.

Even as cross-departmental work increases, another challenge for journalism education will be to identify which data courses need to be framed journalistically and which others can be learned through classes framed within the methodology of other departments. In order to learn statistics, for example, students may be encouraged to register in classes offered by the math, statistics, or even political science department. The principles and objectives of these classes could apply within journalistic work, but that may not always be the case. These classes are often taught from a research or theoretical perspective. A statistics class that emphasizes survey methodology, for example, could be less useful for a journalism student.

Journalists do not often work with samples, but they do work with entire data sets. For data journalism education in particular, a more useful statistics class might be the type of instruction Meyer provided both in college courses and in IRE/NICAR boot camps, using social science to address journalistic challenges. Accommodating both techniques in a research or statistics class could foster collaboration instead of silos. In other instances, outsourcing a course may make sense. Mapping skills necessary for journalists, for example, are the same types of skills necessary for other disciplines in academia.

Yet the task of developing and adopting a data journalism curriculum comes with its own challenges. The high rate of change in digital tools, platforms, and programming languages means that there is more to teach and that classes themselves must be updated frequently. It is difficult to decipher which new techniques are just passing fads and which have the potential to remain relevant for even ten years. For this reason, it is important for classes to be designed so that they teach data and computation as fundamental styles of inquiry. Students can learn enough about the concepts behind a technique to be able to more easily learn new tools that address the technique—as opposed to focusing on the discrete tools used from time to time.

There are exceptions—the Unix command line, for example, has been as fundamental and immutable as any computing tool. This is a text-based application, still favored by developers for many tasks on Mac and Linux systems, for controlling the computer using typed commands instead of a graphical interface. And many of its core utilities remain essentially unchanged since the 1970s. Yet it is far more common to cite such examples as the ActionScript language for Adobe Flash, which was taught at several journalism schools less than a decade ago and is all but abandoned by developers today. The silver lining is that ActionScript shares many features with programming languages such as JavaScript and Python, so it may have offered a path for a student to develop other proficiencies. But it also highlights the importance of selecting techniques for journalism classes with long-term considerations in mind.

1 Pulitzer, “Planning a School of Journalism,” p. 53.

2 Howard, “The Art and Science of Data-Driven Journalism,” p. 4.

3 In later editions, the name changed to The New Precision Journalism (2013).

4 Meyer, Precision Journalism, p. 3.

5 For a more complete look at the long and storied history of computer-assisted reporting, the spring/summer 2015 edition of the IRE Journal provides a detailed and engaging recounting by Jennifer LaFleur, NICAR’s first training director in 1994 and now the senior data editor at the Center for Investigative Reporting/Reveal. Brant Houston details that history in “Fifty Years of Journalism and Data: A Brief History,” Global Investigative Journalism Network, November 12, 2015.

6 Franco Moretti, Graphs, Maps, Trees: Abstract Models for Literary History (London: Verso, 2007) and Dennis Tenen, “Blunt Instrumentalism,” in Debates in the Digital Humanities, forthcoming in 2016, University of Minnesota Press.

7 Folkerts, Hamilton, and Lemann, “Educating Journalists,” p. 4.

8 Ibid., p. 12.

NICAR conferences over time. The conference was not held in 2001 because of 9/11\.

Source: IRE